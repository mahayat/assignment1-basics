{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "770de437",
   "metadata": {},
   "source": [
    "#### 2.1 The Unicode Standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a029e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29275"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord('Áâõ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb456bdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Áâõ'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(29275)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2812d39a",
   "metadata": {},
   "source": [
    "##### Problem (`unicode1`): Understanding Unicode (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98e7d43",
   "metadata": {},
   "source": [
    "_(a) What Unicode character does chr(0) return?_\n",
    "\n",
    "_Answer_: It returns a `str` object representing null character with unicode code point `0`. Here `\\x` is escape sequence, to represent this non-printable charecter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c55cf0bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\x00', 1, str)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0), len(chr(0)), type(chr(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767a4ae3",
   "metadata": {},
   "source": [
    "_(b) How does this character‚Äôs string representation (`__repr__()`) differ from its printed representation?_\n",
    "\n",
    "_Answer_: As we cannot print the null charecter, `repr` shows the `str` representation of this chareter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "462fc8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'\\x00'\n"
     ]
    }
   ],
   "source": [
    "print(repr(chr(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12e7a806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0)) #NULL character is not visible when printed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4869ebcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"'\\\\x00'\", 6, str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repr(chr(0)), len(repr(chr(0))), type(repr(chr(0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb369f3",
   "metadata": {},
   "source": [
    "_(c) What happens when this character occurs in text? It may be helpful to play around with the\n",
    "following in your Python interpreter and see if it matches your expectations:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb47305c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\x00'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e107b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0000\n"
     ]
    }
   ],
   "source": [
    "print(chr(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40468b49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a test\\x00string'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"this is a test\" + chr(0) + \"string\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "95396638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a test\u0000string\n"
     ]
    }
   ],
   "source": [
    "print(\"this is a test\" + chr(0) + \"string\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6d5b5",
   "metadata": {},
   "source": [
    "#### 2.2 Unicode Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49d6e60a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello! „Åì„Çì„Å´„Å°„ÅØ!'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string = \"hello! „Åì„Çì„Å´„Å°„ÅØ!\"\n",
    "test_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "119c9ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'hello! \\xe3\\x81\\x93\\xe3\\x82\\x93\\xe3\\x81\\xab\\xe3\\x81\\xa1\\xe3\\x81\\xaf!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utf8_encoded = test_string.encode(\"utf-8\")\n",
    "utf8_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc80d6b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "print(type(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9e46e2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(int, bytes)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(utf8_encoded[10]), type(utf8_encoded[10:12]) # Single elments of `bytes` object return `int`. Slices return `bytes`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc2625cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104, 101, 108, 108, 111, 33, 32, 227, 129, 147, 227, 130, 147, 227, 129, 171, 227, 129, 161, 227, 129, 175, 33]\n"
     ]
    }
   ],
   "source": [
    "print(list(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa445715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "print(len(test_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20de58fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n"
     ]
    }
   ],
   "source": [
    "print(len(utf8_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf9c7cbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello! „Åì„Çì„Å´„Å°„ÅØ!\n"
     ]
    }
   ],
   "source": [
    "print(utf8_encoded.decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f40db0",
   "metadata": {},
   "source": [
    "##### Problem (`unicode2`): Unicode Encodings (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b4a7c",
   "metadata": {},
   "source": [
    "_(a) What are some reasons to prefer training our tokenizer on UTF-8 encoded bytes, rather than\n",
    "UTF-16 or UTF-32? It may be helpful to compare the output of these encodings for various\n",
    "input strings._\n",
    "\n",
    "_Answer_: Because UTF-16 or UTF-32 occupy 2 bytes and 4 bytes at minimum respectively to represent a charecter. Using byte-level BPE with UTF-16 or UTF-32 means we start with vocab size of 2^16 and 2^32 respectively. Smaller vocabulary = faster training, less memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e68fce07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(256, 65536, 4294967296)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**8, 2**16, 2**32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3e0d8aab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\xff\\xfeh\\x00e\\x00l\\x00l\\x00o\\x00!\\x00 \\x00S0\\x930k0a0o0!\\x00',\n",
       " b'\\xff\\xfe\\x00\\x00h\\x00\\x00\\x00e\\x00\\x00\\x00l\\x00\\x00\\x00l\\x00\\x00\\x00o\\x00\\x00\\x00!\\x00\\x00\\x00 \\x00\\x00\\x00S0\\x00\\x00\\x930\\x00\\x00k0\\x00\\x00a0\\x00\\x00o0\\x00\\x00!\\x00\\x00\\x00')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_string.encode(\"utf-16\"), test_string.encode(\"utf-32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651f0b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[255, 254, 104, 0, 101, 0, 108, 0, 108, 0, 111, 0, 33, 0, 32, 0, 83, 48, 147, 48, 107, 48, 97, 48, 111, 48, 33, 0]\n"
     ]
    }
   ],
   "source": [
    "print(list(test_string.encode(\"utf-16\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7eecc65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 56)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_string.encode(\"utf-16\")), len(test_string.encode(\"utf-32\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89330063",
   "metadata": {},
   "source": [
    "_(b) Consider the following (incorrect) function, which is intended to decode a UTF-8 byte string into a Unicode string. Why is this function incorrect? Provide an example of an input byte string that yields incorrect results._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e8aa1ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_utf8_bytes_to_str_wrong(bytestring: bytes):\n",
    "    return \"\".join([bytes([b]).decode(\"utf-8\") for b in bytestring])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44858f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_utf8_bytes_to_str_wrong(\"hello\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e9ed4b",
   "metadata": {},
   "source": [
    "_(c) Give a two byte sequence that does not decode to any Unicode character(s)._\n",
    "\n",
    "_Answer_: UTF-8 can combine multiple bytes from one charecter. In those cases the bytes must follow a pattern that individually does not translate to any valid charecter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3873ad4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[240, 159, 152, 128]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"üòÄ\".encode(\"utf-8\")\n",
    "list(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "63312f5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(bytes, int, bytes)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(test), type(test[0]), type(bytes(test[0]))\n",
    "\n",
    "# When we access a byte object, we get an integer representing the byte value at that position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6181709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xf0\\x9f\\x98\\x80'\n"
     ]
    }
   ],
   "source": [
    "print(test)  # b'\\xf0\\x9f\\x98\\x80'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "399b8a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UnicodeDecodeError: 'utf-8' codec can't decode byte 0xf0 in position 0: unexpected end of data\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    decode_utf8_bytes_to_str_wrong(test)\n",
    "except UnicodeDecodeError as e:\n",
    "    print(\"UnicodeDecodeError:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2415d050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'\\xc3\\xa9'\n"
     ]
    }
   ],
   "source": [
    "test = \"√©\".encode(\"utf-8\")\n",
    "print(test)  # b'\\xc3\\xa9'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4eca7928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'e', b'\\xc3\\xa9')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"e\".encode(\"utf-8\"), \"√©\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ebf05a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "## To add: \n",
    "## immutable nature of bytes object\n",
    "## max and min byte size of different unicode encodings\n",
    "## bytearrays\n",
    "## unicode pattern for muli-byte charecters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa86b51",
   "metadata": {},
   "source": [
    "#### 2.4 BPE Tokenizer Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e59fec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/openai/tiktoken/pull/234/files\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8dc296ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ',', ' world', '!', ' C', \"'\", 'est', ' la', ' vie', '.', ' Áâõ', '.', ' ‡¶¨', '‡¶æ‡¶Ç', '‡¶≤', '‡¶æ', ' ‡¶Ü‡¶Æ', '‡¶æ', '‡¶∞', ' ‡¶≠', '‡¶æ', '‡¶∑', '‡¶æ‡•§']\n"
     ]
    }
   ],
   "source": [
    "import regex as re\n",
    "print(re.findall(PAT, \"Hello, world! C'est la vie. Áâõ. ‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶∞ ‡¶≠‡¶æ‡¶∑‡¶æ‡•§\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32c72604",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('BA', 'A')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max([(\"A\", \"B\"), (\"A\", \"C\"), (\"B\", \"ZZ\"), (\"BA\", \"A\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17367dab",
   "metadata": {},
   "source": [
    "##### Example (`bpe_example`): BPE training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e32fc904",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = \"\"\"low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\"\"\"\n",
    "\n",
    "# try the same with Bangla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e8ee1ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "low low low low low\n",
      "lower lower widest widest widest\n",
      "newest newest newest newest newest newest\n"
     ]
    }
   ],
   "source": [
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c99cbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.bpe_example import pretokenize, count_pairs, merge_pair, train_bpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b18a00be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(b'l', b'o', b'w'): 5,\n",
       " (b'l', b'o', b'w', b'e', b'r'): 2,\n",
       " (b'w', b'i', b'd', b'e', b's', b't'): 3,\n",
       " (b'n', b'e', b'w', b'e', b's', b't'): 6}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freqs = pretokenize(f)\n",
    "word_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5d88b190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(b'l', b'o'): 7, (b'o', b'w'): 7, (b'w', b'e'): 8, (b'e', b'r'): 2, (b'w', b'i'): 3, (b'i', b'd'): 3, (b'd', b'e'): 3, (b'e', b's'): 9, (b's', b't'): 9, (b'n', b'e'): 6, (b'e', b'w'): 6}\n"
     ]
    }
   ],
   "source": [
    "pairs = count_pairs(word_freqs)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e9e42221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge_pair(pairs, word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56bb9ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent pair: (b's', b't') with count 9\n"
     ]
    }
   ],
   "source": [
    "# Get the most frequent pair\n",
    "best_pair = max(pairs.items(), key=lambda x: (x[1], x[0]))\n",
    "pair, count = best_pair\n",
    "\n",
    "print(f\"Most frequent pair: {pair} with count {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "131db02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(b'l', b'o', b'w'): 5, (b'l', b'o', b'w', b'e', b'r'): 2, (b'w', b'i', b'd', b'e', b'st'): 3, (b'n', b'e', b'w', b'e', b'st'): 6}\n"
     ]
    }
   ],
   "source": [
    "# Now merge that specific pair\n",
    "new_word_freqs = merge_pair(word_freqs, pair)  # Pass 'pair', not 'pairs'\n",
    "print(new_word_freqs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6a0fef82",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges = train_bpe(f, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef773c9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b's', b't'),\n",
       " (b'e', b'st'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'ow'),\n",
       " (b'w', b'est'),\n",
       " (b'n', b'e')]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9b4fb37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(b'\\xc3\\x88', b'\\xc3')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(200).encode(), bytes([chr(200).encode()[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "87014295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, '√à')"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chr(200).encode()), chr(200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2778a81",
   "metadata": {},
   "source": [
    "#### 2.5 Experimenting with BPE Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c873fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cs336_basics.tobenamed import train_bpe#, get_all_pretokens, get_pretoken_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4ce82bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"data/TinyStoriesV2-GPT4-valid.txt\"\n",
    "vocab_size = 10_000\n",
    "special_tokens = [\"<|endoftext|>\"]\n",
    "# num_processes = 20\n",
    "# split_special_token = \"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "73bbc3fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tokens, pre_tokens_bytes = train_bpe(input_path, vocab_size, special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "580758b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " ' don',\n",
       " \"'t\",\n",
       " ' have',\n",
       " ' to',\n",
       " ' be',\n",
       " ' scared',\n",
       " ' of',\n",
       " ' the',\n",
       " ' loud',\n",
       " ' dog',\n",
       " ',',\n",
       " ' I',\n",
       " \"'ll\",\n",
       " ' protect',\n",
       " ' you',\n",
       " '\".',\n",
       " ' The',\n",
       " ' mole',\n",
       " ' felt',\n",
       " ' so',\n",
       " ' safe',\n",
       " ' with',\n",
       " ' the',\n",
       " ' little',\n",
       " ' girl',\n",
       " '.',\n",
       " ' She',\n",
       " ' was',\n",
       " ' very',\n",
       " ' kind',\n",
       " ' and',\n",
       " ' the',\n",
       " ' mole',\n",
       " ' soon',\n",
       " ' came',\n",
       " ' to',\n",
       " ' trust',\n",
       " ' her',\n",
       " '.',\n",
       " ' He',\n",
       " ' leaned',\n",
       " ' against',\n",
       " ' her',\n",
       " ' and',\n",
       " ' she',\n",
       " ' kept',\n",
       " ' him',\n",
       " ' safe',\n",
       " '.',\n",
       " ' The',\n",
       " ' mole',\n",
       " ' had',\n",
       " ' found',\n",
       " ' his',\n",
       " ' best',\n",
       " ' friend',\n",
       " '.',\n",
       " '\\n',\n",
       " '<|',\n",
       " 'endoftext',\n",
       " '|>',\n",
       " '\\n',\n",
       " 'Once',\n",
       " ' upon',\n",
       " ' a',\n",
       " ' time',\n",
       " ',',\n",
       " ' in',\n",
       " ' a',\n",
       " ' warm',\n",
       " ' and',\n",
       " ' sunny',\n",
       " ' place',\n",
       " ',',\n",
       " ' there',\n",
       " ' was',\n",
       " ' a',\n",
       " ' big',\n",
       " ' pit',\n",
       " '.',\n",
       " ' A',\n",
       " ' little',\n",
       " ' boy',\n",
       " ' named',\n",
       " ' Tom',\n",
       " ' liked',\n",
       " ' to',\n",
       " ' play',\n",
       " ' near',\n",
       " ' the',\n",
       " ' pit',\n",
       " '.',\n",
       " ' One',\n",
       " ' day',\n",
       " ',',\n",
       " ' Tom',\n",
       " ' lost',\n",
       " ' his',\n",
       " ' red',\n",
       " ' ball',\n",
       " '.',\n",
       " ' He',\n",
       " ' was',\n",
       " ' very',\n",
       " ' sad',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Tom',\n",
       " ' asked',\n",
       " ' his',\n",
       " ' friend',\n",
       " ',',\n",
       " ' Sam',\n",
       " ',',\n",
       " ' to',\n",
       " ' help',\n",
       " ' him',\n",
       " ' search',\n",
       " ' for',\n",
       " ' the',\n",
       " ' ball',\n",
       " '.',\n",
       " ' They',\n",
       " ' looked',\n",
       " ' high',\n",
       " ' and',\n",
       " ' low',\n",
       " ',',\n",
       " ' but',\n",
       " ' they',\n",
       " ' could',\n",
       " ' not',\n",
       " ' find',\n",
       " ' the',\n",
       " ' ball',\n",
       " '.',\n",
       " ' Tom',\n",
       " ' said',\n",
       " ',',\n",
       " ' \"',\n",
       " 'I',\n",
       " ' think',\n",
       " ' my',\n",
       " ' ball',\n",
       " ' fell',\n",
       " ' into',\n",
       " ' the',\n",
       " ' pit',\n",
       " '.\"',\n",
       " '\\n',\n",
       " 'Sam',\n",
       " ' and',\n",
       " ' Tom',\n",
       " ' went',\n",
       " ' close',\n",
       " ' to',\n",
       " ' the',\n",
       " ' pit',\n",
       " '.',\n",
       " ' They',\n",
       " ' were',\n",
       " ' scared',\n",
       " ',',\n",
       " ' but',\n",
       " ' they',\n",
       " ' wanted',\n",
       " ' to',\n",
       " ' find',\n",
       " ' the',\n",
       " ' red',\n",
       " ' ball',\n",
       " '.',\n",
       " ' They',\n",
       " ' looked',\n",
       " ' into',\n",
       " ' the',\n",
       " ' pit',\n",
       " ',',\n",
       " ' but',\n",
       " ' it',\n",
       " ' was',\n",
       " ' too',\n",
       " ' dark',\n",
       " ' to',\n",
       " ' see',\n",
       " '.',\n",
       " ' Tom',\n",
       " ' said',\n",
       " ',',\n",
       " ' \"',\n",
       " 'We',\n",
       " ' must',\n",
       " ' go',\n",
       " ' in',\n",
       " ' and',\n",
       " ' search',\n",
       " ' for',\n",
       " ' my',\n",
       " ' ball',\n",
       " '.\"',\n",
       " '\\n',\n",
       " 'They',\n",
       " ' went',\n",
       " ' into',\n",
       " ' the',\n",
       " ' pit',\n",
       " ' to',\n",
       " ' search',\n",
       " '.',\n",
       " ' It',\n",
       " ' was',\n",
       " ' dark',\n",
       " ' and',\n",
       " ' scary',\n",
       " '.',\n",
       " ' They',\n",
       " ' could',\n",
       " ' not',\n",
       " ' find',\n",
       " ' the',\n",
       " ' ball',\n",
       " '.',\n",
       " ' They',\n",
       " ' tried',\n",
       " ' to',\n",
       " ' get',\n",
       " ' out',\n",
       " ',',\n",
       " ' but',\n",
       " ' the',\n",
       " ' pit',\n",
       " ' was',\n",
       " ' too',\n",
       " ' deep',\n",
       " '.',\n",
       " ' Tom',\n",
       " ' and',\n",
       " ' Sam',\n",
       " ' were',\n",
       " ' stuck',\n",
       " ' in',\n",
       " ' the',\n",
       " ' pit',\n",
       " '.',\n",
       " ' They',\n",
       " ' called',\n",
       " ' for',\n",
       " ' help',\n",
       " ',',\n",
       " ' but',\n",
       " ' no',\n",
       " ' one',\n",
       " ' could',\n",
       " ' hear',\n",
       " ' them',\n",
       " '.',\n",
       " ' They',\n",
       " ' were',\n",
       " ' sad',\n",
       " ' and',\n",
       " ' scared',\n",
       " ',',\n",
       " ' and',\n",
       " ' they',\n",
       " ' never',\n",
       " ' got',\n",
       " ' out',\n",
       " ' of',\n",
       " ' the',\n",
       " ' pit',\n",
       " '.',\n",
       " '\\n',\n",
       " '<|',\n",
       " 'endoftext',\n",
       " '|>',\n",
       " '\\n\\n',\n",
       " '\\n',\n",
       " 'Tom',\n",
       " ' and',\n",
       " ' Lily',\n",
       " ' were',\n",
       " ' playing',\n",
       " ' with',\n",
       " ' their',\n",
       " ' toys',\n",
       " ' in',\n",
       " ' the',\n",
       " ' living',\n",
       " ' room',\n",
       " '.',\n",
       " ' They',\n",
       " ' liked',\n",
       " ' to',\n",
       " ' build',\n",
       " ' towers',\n",
       " ' and',\n",
       " ' bridges',\n",
       " ' with',\n",
       " ' their',\n",
       " ' blocks',\n",
       " ' and',\n",
       " ' cars',\n",
       " '.',\n",
       " ' Tom',\n",
       " ' was',\n",
       " ' very',\n",
       " ' proud',\n",
       " ' of',\n",
       " ' his',\n",
       " ' tall',\n",
       " ' tower',\n",
       " '.',\n",
       " ' He',\n",
       " ' wanted',\n",
       " ' to',\n",
       " ' make',\n",
       " ' it',\n",
       " ' even',\n",
       " ' taller',\n",
       " ',',\n",
       " ' so',\n",
       " ' he',\n",
       " ' reached',\n",
       " ' for',\n",
       " ' more',\n",
       " ' blocks',\n",
       " '.',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'Tom',\n",
       " ',',\n",
       " ' can',\n",
       " ' I',\n",
       " ' have',\n",
       " ' some',\n",
       " ' blocks',\n",
       " ' too',\n",
       " '?\"',\n",
       " ' Lily',\n",
       " ' asked',\n",
       " '.',\n",
       " ' She',\n",
       " ' wanted',\n",
       " ' to',\n",
       " ' make',\n",
       " ' a',\n",
       " ' bridge',\n",
       " ' for',\n",
       " ' her',\n",
       " ' cars',\n",
       " '.',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'No',\n",
       " ',',\n",
       " ' these',\n",
       " ' are',\n",
       " ' mine',\n",
       " '.',\n",
       " ' Go',\n",
       " ' find',\n",
       " ' your',\n",
       " ' own',\n",
       " ',\"',\n",
       " ' Tom',\n",
       " ' said',\n",
       " '.',\n",
       " ' He',\n",
       " ' did',\n",
       " ' not',\n",
       " ' want',\n",
       " ' to',\n",
       " ' share',\n",
       " ' with',\n",
       " ' his',\n",
       " ' sister',\n",
       " '.',\n",
       " ' He',\n",
       " ' pulled',\n",
       " ' the',\n",
       " ' blocks',\n",
       " ' closer',\n",
       " ' to',\n",
       " ' him',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Lily',\n",
       " ' felt',\n",
       " ' sad',\n",
       " ' and',\n",
       " ' angry',\n",
       " '.',\n",
       " ' She',\n",
       " ' did',\n",
       " ' not',\n",
       " ' think',\n",
       " ' Tom',\n",
       " ' was',\n",
       " ' being',\n",
       " ' nice',\n",
       " '.',\n",
       " ' She',\n",
       " ' looked',\n",
       " ' at',\n",
       " ' his',\n",
       " ' tower',\n",
       " ' and',\n",
       " ' had',\n",
       " ' an',\n",
       " ' idea',\n",
       " '.',\n",
       " ' She',\n",
       " ' decided',\n",
       " ' to',\n",
       " ' pull',\n",
       " ' one',\n",
       " ' of',\n",
       " ' the',\n",
       " ' blocks',\n",
       " ' at',\n",
       " ' the',\n",
       " ' bottom',\n",
       " ' of',\n",
       " ' the',\n",
       " ' tower',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Suddenly',\n",
       " ',',\n",
       " ' the',\n",
       " ' tower',\n",
       " ' fell',\n",
       " ' down',\n",
       " ' with',\n",
       " ' a',\n",
       " ' loud',\n",
       " ' crash',\n",
       " '.',\n",
       " ' All',\n",
       " ' the',\n",
       " ' blocks',\n",
       " ' and',\n",
       " ' cars',\n",
       " ' scattered',\n",
       " ' on',\n",
       " ' the',\n",
       " ' floor',\n",
       " '.',\n",
       " ' Tom',\n",
       " ' and',\n",
       " ' Lily',\n",
       " ' were',\n",
       " ' shocked',\n",
       " '.',\n",
       " ' They',\n",
       " ' felt',\n",
       " ' the',\n",
       " ' floor',\n",
       " ' shake',\n",
       " ' and',\n",
       " ' heard',\n",
       " ' a',\n",
       " ' rumble',\n",
       " '.',\n",
       " ' It',\n",
       " ' was',\n",
       " ' an',\n",
       " ' earthquake',\n",
       " '!',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'Mommy',\n",
       " '!',\n",
       " ' Daddy',\n",
       " '!\"',\n",
       " ' they',\n",
       " ' cried',\n",
       " '.',\n",
       " ' They',\n",
       " ' were',\n",
       " ' scared',\n",
       " ' and',\n",
       " ' ran',\n",
       " ' to',\n",
       " ' their',\n",
       " ' parents',\n",
       " ',',\n",
       " ' who',\n",
       " ' were',\n",
       " ' in',\n",
       " ' the',\n",
       " ' kitchen',\n",
       " '.',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'Are',\n",
       " ' you',\n",
       " ' okay',\n",
       " ',',\n",
       " ' kids',\n",
       " '?\"',\n",
       " ' Mommy',\n",
       " ' asked',\n",
       " '.',\n",
       " ' She',\n",
       " ' hugged',\n",
       " ' them',\n",
       " ' and',\n",
       " ' checked',\n",
       " ' if',\n",
       " ' they',\n",
       " ' were',\n",
       " ' hurt',\n",
       " '.',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'We',\n",
       " \"'re\",\n",
       " ' okay',\n",
       " ',',\n",
       " ' Mommy',\n",
       " '.',\n",
       " ' But',\n",
       " ' our',\n",
       " ' toys',\n",
       " ' are',\n",
       " ' broken',\n",
       " ',\"',\n",
       " ' Lily',\n",
       " ' said',\n",
       " '.',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'I',\n",
       " \"'m\",\n",
       " ' sorry',\n",
       " ',',\n",
       " ' Lily',\n",
       " '.',\n",
       " ' But',\n",
       " ' toys',\n",
       " ' are',\n",
       " ' not',\n",
       " ' important',\n",
       " '.',\n",
       " ' You',\n",
       " ' are',\n",
       " ' important',\n",
       " '.',\n",
       " ' We',\n",
       " ' are',\n",
       " ' safe',\n",
       " ' and',\n",
       " ' together',\n",
       " '.',\n",
       " ' That',\n",
       " \"'s\",\n",
       " ' what',\n",
       " ' matters',\n",
       " ',\"',\n",
       " ' Mommy',\n",
       " ' said',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Tom',\n",
       " ' felt',\n",
       " ' sorry',\n",
       " ' for',\n",
       " ' what',\n",
       " ' he',\n",
       " ' did',\n",
       " '.',\n",
       " ' He',\n",
       " ' realized',\n",
       " ' he',\n",
       " ' was',\n",
       " ' selfish',\n",
       " ' and',\n",
       " ' mean',\n",
       " ' to',\n",
       " ' his',\n",
       " ' sister',\n",
       " '.',\n",
       " ' He',\n",
       " ' saw',\n",
       " ' how',\n",
       " ' scared',\n",
       " ' she',\n",
       " ' was',\n",
       " ' during',\n",
       " ' the',\n",
       " ' earthquake',\n",
       " '.',\n",
       " ' He',\n",
       " ' wanted',\n",
       " ' to',\n",
       " ' make',\n",
       " ' her',\n",
       " ' happy',\n",
       " '.',\n",
       " '\\n',\n",
       " '\"',\n",
       " 'Lily',\n",
       " ',',\n",
       " ' I',\n",
       " \"'m\",\n",
       " ' sorry',\n",
       " ' I',\n",
       " ' did',\n",
       " ' not',\n",
       " ' share',\n",
       " ' with',\n",
       " ' you',\n",
       " '.',\n",
       " ' You',\n",
       " ' can',\n",
       " ' have',\n",
       " ' all',\n",
       " ' the',\n",
       " ' blocks',\n",
       " ' you',\n",
       " ' want',\n",
       " '.',\n",
       " ' I',\n",
       " ' love',\n",
       " ' you',\n",
       " ',',\n",
       " ' sister',\n",
       " ',\"',\n",
       " ' Tom',\n",
       " ' said',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Lily',\n",
       " ' smiled',\n",
       " ' and',\n",
       " ' hugged',\n",
       " ' him',\n",
       " '.',\n",
       " ' She',\n",
       " ' forgave',\n",
       " ' him',\n",
       " ' and',\n",
       " ' thanked',\n",
       " ' him',\n",
       " '.',\n",
       " ' She',\n",
       " ' loved',\n",
       " ' him',\n",
       " ' too',\n",
       " '.',\n",
       " '\\n',\n",
       " 'They',\n",
       " ' went',\n",
       " ' back',\n",
       " ' to',\n",
       " ' the',\n",
       " ' living',\n",
       " ' room',\n",
       " ' and',\n",
       " ' cleaned',\n",
       " ' up',\n",
       " ' their',\n",
       " ' toys',\n",
       " '.',\n",
       " ' They',\n",
       " ' decided',\n",
       " ' to',\n",
       " ' build',\n",
       " ' something',\n",
       " ' together',\n",
       " '.',\n",
       " ' They',\n",
       " ' made',\n",
       " ' a',\n",
       " ' big',\n",
       " ' house',\n",
       " ' with',\n",
       " ' a',\n",
       " ' garden',\n",
       " ' and',\n",
       " ' a',\n",
       " ' fence',\n",
       " '.',\n",
       " ' They',\n",
       " ' put',\n",
       " ' their',\n",
       " ' cars',\n",
       " ' and',\n",
       " ' dolls',\n",
       " ' inside',\n",
       " '.',\n",
       " ' They',\n",
       " ' were',\n",
       " ' happy',\n",
       " ' and',\n",
       " ' proud',\n",
       " ' of',\n",
       " ' their',\n",
       " ' work',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Mommy',\n",
       " ' and',\n",
       " ' Daddy',\n",
       " ' came',\n",
       " ' to',\n",
       " ' see',\n",
       " ' their',\n",
       " ' house',\n",
       " '.',\n",
       " ' They',\n",
       " ' praised',\n",
       " ' them',\n",
       " ' and',\n",
       " ' gave',\n",
       " ' them',\n",
       " ' a',\n",
       " ' treat',\n",
       " '.',\n",
       " ' It',\n",
       " ' was',\n",
       " ' a',\n",
       " ' lemon',\n",
       " ' cake',\n",
       " '.',\n",
       " ' It',\n",
       " ' was',\n",
       " ' sour',\n",
       " ',',\n",
       " ' but',\n",
       " ' they',\n",
       " ' liked',\n",
       " ' it',\n",
       " '.',\n",
       " ' They',\n",
       " ' learned',\n",
       " ' that',\n",
       " ' sharing',\n",
       " ' is',\n",
       " ' caring',\n",
       " ',',\n",
       " ' and',\n",
       " ' that',\n",
       " ' family',\n",
       " ' is',\n",
       " ' sweet',\n",
       " '.',\n",
       " '\\n',\n",
       " '<|',\n",
       " 'endoftext',\n",
       " '|>',\n",
       " '\\n',\n",
       " '\\n',\n",
       " 'Once',\n",
       " ' upon',\n",
       " ' a',\n",
       " ' time',\n",
       " ' there',\n",
       " ' was',\n",
       " ' a',\n",
       " ' little',\n",
       " ' girl',\n",
       " ' named',\n",
       " ' Lucy',\n",
       " '.',\n",
       " ' She',\n",
       " ' loved',\n",
       " ' to',\n",
       " ' go',\n",
       " ' to',\n",
       " ' the',\n",
       " ' store',\n",
       " ' to',\n",
       " ' buy',\n",
       " ' sweets',\n",
       " ' with',\n",
       " ' her',\n",
       " ' mom',\n",
       " ' and',\n",
       " ' dad',\n",
       " '.',\n",
       " ' On',\n",
       " ' this',\n",
       " ' special',\n",
       " ' day',\n",
       " ',',\n",
       " ' Lucy',\n",
       " ' entered',\n",
       " ' the',\n",
       " ' store',\n",
       " ' with',\n",
       " ' her',\n",
       " ' mom',\n",
       " ' and',\n",
       " ' dad',\n",
       " ',',\n",
       " ' feeling',\n",
       " ' so',\n",
       " ' excited',\n",
       " '.',\n",
       " '\\n',\n",
       " 'As',\n",
       " ' they',\n",
       " ' were',\n",
       " ' looking',\n",
       " ' around',\n",
       " ',',\n",
       " ' Lucy',\n",
       " ' noticed',\n",
       " ' a',\n",
       " ' little',\n",
       " ' girl',\n",
       " ' playing',\n",
       " ' with',\n",
       " ' a',\n",
       " ' toy',\n",
       " ' in',\n",
       " ' the',\n",
       " ' corner',\n",
       " ' of',\n",
       " ' the',\n",
       " ' store',\n",
       " '.',\n",
       " ' She',\n",
       " ' gasped',\n",
       " ' in',\n",
       " ' excitement',\n",
       " ' and',\n",
       " ' ran',\n",
       " ' towards',\n",
       " ' her',\n",
       " '.',\n",
       " ' Lucy',\n",
       " ' asked',\n",
       " ' if',\n",
       " ' she',\n",
       " ' could',\n",
       " ' play',\n",
       " ' too',\n",
       " ' but',\n",
       " ' the',\n",
       " ' little',\n",
       " ' girl',\n",
       " ' said',\n",
       " ' no',\n",
       " '.',\n",
       " ' She',\n",
       " ' was',\n",
       " ' rather',\n",
       " ' grumpy',\n",
       " ' and',\n",
       " ' was',\n",
       " ' not',\n",
       " ' in',\n",
       " ' the',\n",
       " ' mood',\n",
       " ' to',\n",
       " ' play',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Lucy',\n",
       " \"'s\",\n",
       " ' mom',\n",
       " ' saw',\n",
       " ' what',\n",
       " ' was',\n",
       " ' going',\n",
       " ' on',\n",
       " ' and',\n",
       " ' told',\n",
       " ' Lucy',\n",
       " ',',\n",
       " ' \"',\n",
       " 'Let',\n",
       " \"'s\",\n",
       " ' try',\n",
       " ' to',\n",
       " ' be',\n",
       " ' peaceful',\n",
       " ' and',\n",
       " ' kind',\n",
       " ' to',\n",
       " ' her',\n",
       " '.',\n",
       " ' Have',\n",
       " ' patience',\n",
       " ' and',\n",
       " ' understanding',\n",
       " '.',\n",
       " ' Together',\n",
       " ',',\n",
       " ' you',\n",
       " ' can',\n",
       " ' both',\n",
       " ' be',\n",
       " ' happy',\n",
       " '!\"',\n",
       " '\\n',\n",
       " 'So',\n",
       " ',',\n",
       " ' Lucy',\n",
       " ' smiled',\n",
       " ' at',\n",
       " ' the',\n",
       " ' girl',\n",
       " ' and',\n",
       " ' said',\n",
       " ',',\n",
       " ' \"',\n",
       " 'Can',\n",
       " ' we',\n",
       " ' play',\n",
       " ' together',\n",
       " '?\"',\n",
       " ' The',\n",
       " ' little',\n",
       " ' girl',\n",
       " ' softened',\n",
       " ' and',\n",
       " ' smiled',\n",
       " ' back',\n",
       " '.',\n",
       " ' She',\n",
       " ' agreed',\n",
       " ' to',\n",
       " ' share',\n",
       " ' the',\n",
       " ' toy',\n",
       " ' and',\n",
       " ' even',\n",
       " ' let',\n",
       " ' Lucy',\n",
       " ' have',\n",
       " ' a',\n",
       " ' turn',\n",
       " ' first',\n",
       " '.',\n",
       " '\\n',\n",
       " 'Lucy',\n",
       " ' and',\n",
       " ' the',\n",
       " ' little',\n",
       " ' girl',\n",
       " ' played',\n",
       " ' together',\n",
       " ' happily',\n",
       " '.',\n",
       " ' In',\n",
       " ' the',\n",
       " ' end',\n",
       " ',',\n",
       " ' they',\n",
       " ' both',\n",
       " ' learnt',\n",
       " ' an',\n",
       " ' important',\n",
       " ' lesson',\n",
       " ':',\n",
       " ' be',\n",
       " ' peaceful',\n",
       " ',',\n",
       " ' kind',\n",
       " ',',\n",
       " ' and',\n",
       " ' understanding',\n",
       " ' when',\n",
       " ' faced',\n",
       " ' with',\n",
       " ' a',\n",
       " ' conflict',\n",
       " '.',\n",
       " ' And',\n",
       " ' that',\n",
       " ' is',\n",
       " ' why',\n",
       " ' Lucy',\n",
       " ' and',\n",
       " ' the',\n",
       " ' little',\n",
       " ' girl',\n",
       " ' became',\n",
       " ' great',\n",
       " ' friends',\n",
       " '.',\n",
       " '\\n',\n",
       " '<|',\n",
       " 'endoftext',\n",
       " '|>',\n",
       " '\\n',\n",
       " 'One',\n",
       " ' morning',\n",
       " ',',\n",
       " ' a',\n",
       " ' cat',\n",
       " ' named',\n",
       " ' Tom',\n",
       " ' woke',\n",
       " ' up',\n",
       " ...]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "47d21dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk = get_chunks(input_path, split_special_token, num_processes)\n",
    "# stories = split_chunk(chunk, special_tokens)\n",
    "# all_pretokens = [token for story in stories for token in re.findall(PAT, story)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "864b0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_pretokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ae935119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# del PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a2aef880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "92d4a17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_tokens = get_all_pretokens(input_path, vocab_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ee190f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretoken_encodings = get_pretoken_counter(all_tokens)\n",
    "# # pretoken_encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbca7c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_freq = dict(sorted(pretoken_encodings.items(), key=lambda item: item[1], reverse=True))\n",
    "# # sorted_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e82bade0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_freq[(b' ', b't', b'h', b'e')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e00ae2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk = get_chunks(input_path = 'data/TinyStoriesV2-GPT4-valid.txt', \n",
    "#            split_special_token = \"<|endoftext|>\", \n",
    "#            num_processes = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "af18f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff070661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb37be75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# special_tokens = [\"<|endoftext|>\"]\n",
    "# stories = split_chunk(chunk, special_tokens)\n",
    "# all_pretokens = [token for story in stories for token in re.findall(PAT, story)]\n",
    "# len(all_pretokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9986ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import defaultdict\n",
    "# freq = defaultdict(int)\n",
    "\n",
    "# for pretoken in all_pretokens:\n",
    "#     byte_tuple = tuple(bytes([b]) for b in pretoken.encode('utf-8'))\n",
    "#     freq[byte_tuple] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9335becd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sorted_freq = dict(sorted(freq.items(), key=lambda item: item[1], reverse=True))\n",
    "# sorted_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ed74ef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(chunk)\n",
    "# print(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2723a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cs336_basics.pretokenization_example import find_chunk_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7507030d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = 'data/TinyStoriesV2-GPT4-valid.txt'\n",
    "\n",
    "# ## Usage\n",
    "# with open(file_path, \"rb\") as f:\n",
    "#     num_processes = 10\n",
    "#     boundaries = find_chunk_boundaries(f, num_processes, b\"<|endoftext|>\")\n",
    "#     print(boundaries)\n",
    "#     # The following is a serial implementation, but you can parallelize this\n",
    "#     # by sending each start/end pair to a set of processes.\n",
    "#     for start, end in zip(boundaries[:-1], boundaries[1:]):\n",
    "#         f.seek(start)\n",
    "#         chunk = f.read(end - start).decode(\"utf-8\", errors=\"ignore\")\n",
    "#         # Run pre-tokenization on your chunk and store the counts for each pre-token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "75d030fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # chunk\n",
    "# end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "108e9718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk.find(\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8d768b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# b\"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1aa6887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"<|endoftext|>\".encode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "98ccb25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunk[720:780]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "d4f6ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# special_tokens = [\"<|endoftext|>\"]#, \"<|startoftext|>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "a8e8ca52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = re.split(special_tokens[0], chunk[:800])\n",
    "# result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "21a60af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pattern = \"|\".join(re.escape(token) for token in special_tokens)\n",
    "# print(pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cb33a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# result = re.split(pattern, chunk)\n",
    "# len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "be147f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(result[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ae768a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(b\"<|endoftext|>\".decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8022e18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chunk[:810])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22176a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(chunk[:2000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f07aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336-basics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
